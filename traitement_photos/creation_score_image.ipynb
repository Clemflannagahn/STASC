{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création des constantes et des quelques données nécessaires aux traitements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path_to_csv = \"C:/Users/maila/Documents/Centrale Nantes/EI2/INFOIA/STASC/Data_Challenge/_classified/train\"\n",
    "\n",
    "# On récup la liste de l'ensemble des fichiers .csv créées à l'aide du modèle de DL RoomNet que j'ai adapté ici\n",
    "list_of_files = os.listdir(path_to_csv)\n",
    "\n",
    "CLASS = ['Backyard', 'Bathroom', 'Bedroom', 'Frontyard', 'Kitchen', 'LivingRoom']\n",
    "POND = [2, 0.5, 1, 1.5, 1.5, 2]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traitement des .xls pour obtenir un unique Dataframe avec un score en fonction des images pour chaque annonce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n"
     ]
    }
   ],
   "source": [
    "# création du dictionnaire final, qui contiendra toutes les données des excels et les scores (sans les valeurs de fiabilité)\n",
    "donnees_finales = pd.DataFrame({'id_annonce':[],'Backyard' : [], 'Bathroom' : [], 'Bedroom' : [], 'Frontyard' : [],'Kitchen' : [], 'LivingRoom' : [],'score':[]})\n",
    "dict_operations = {'Value_Count': ('PREDICTED_LABEL', 'count'), 'average_fiability': ('Unnamed: 2', 'mean')} # dictionnaire qui contient les opérations à effectuer sur les colonnes des dataframes qu'on récup\n",
    "\n",
    "i=0\n",
    "for file in list_of_files:\n",
    "    if i%1000 == 0: # compteur pour suivre l'avancement du traitement\n",
    "        print(i)\n",
    "    i = i+1\n",
    "    A = pd.read_excel(path_to_csv + \"/\" + file)\n",
    "    elem_presents = A.groupby('PREDICTED_LABEL', as_index=False).agg(**dict_operations) # on crée un dataframe avec le count des différentes classes et la fiabilité moyenne par classe\n",
    "\n",
    "    dict_temp = {} # création du dictionnaire temporaire qui contiendra la valeurs avant de les mettre dans le dataframe final\n",
    "    score = 0 # initialisation du score\n",
    "    for elem in elem_presents['PREDICTED_LABEL']:\n",
    "        number_of_elem = elem_presents['Value_Count'][elem_presents['PREDICTED_LABEL'] == elem].tolist()[0] # on récupère le nombre d'éléments de la classe\n",
    "        average_fiability = elem_presents['average_fiability'][elem_presents['PREDICTED_LABEL'] == elem].tolist()[0] # on récupère la fiabilité moyenne de la classe\n",
    "        dict_temp[elem] = [number_of_elem] # on met le nombre d'éléments de la classe dans le dictionnaire\n",
    "        score = score + number_of_elem*POND[CLASS.index(elem)]*average_fiability # on calcule le score en multipliant notre pondération et la fiabilité moyenne de la classe\n",
    "\n",
    "    score = [score]\n",
    "    dict_temp['score'] = score # on ajoute le score au dictionnaire\n",
    "    dict_temp['id_annonce'] = file[4:-12] # on ajoute l'id de l'annonce au dictionnaire\n",
    "\n",
    "    temp = pd.DataFrame(dict_temp)\n",
    "    donnees_finales = pd.concat([donnees_finales,temp],ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "donnees_finales.to_csv('donnees_finales_images_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_test = pd.read_csv('donnees_finales_images_test.csv')\n",
    "image_train = pd.read_csv('donnees_finales_images_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concaténation de test et train\n",
    "image = pd.concat([image_test,image_train],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('../Prix_metre/X_avec_prix_complet2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge X et image sur l'id_annonce\n",
    "X = pd.merge(X, image, on='id_annonce', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_csv('X_avec_prix_et_images.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Projet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:29:51) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63e22c97e4ed8d431fc724019dffa39fd8054b2d28cf3e66fed132b173246b98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
