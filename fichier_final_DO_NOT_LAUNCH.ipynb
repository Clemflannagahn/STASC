{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports nécessaires pour tout le reste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None # supprime certains warnings\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from statistics import mean\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Récupération du Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> L'ordre des opérations ici ne correspond pas à l'ordre dans lequel les opérations ont réellement été faites (les récupérations de datasets du gouvernement ayant été réalisés au fur et à mesure du rajout de traitements) mais représente une version la plus compréhensible possible"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> On récupère le fichier contenant l'ensemble des données finales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_AC = pd.read_csv(\"../traitement_photos/X_avec_prix_et_images_complet_et_inflation.csv\")\n",
    "\n",
    "X_train_site = pd.read_csv(\"./X_train_J01Z4CN.csv\")\n",
    "X_test_site = pd.read_csv(\"./X_test_BEhvxAN.csv\")\n",
    "\n",
    "# X_AC correspond à l'ensemble des données test et train compris (X_AC = X_AC)\n",
    "X_AC = pd.concat([X_train_site,X_test_site],ignore_index=True)\n",
    "\n",
    "# X_train = pd.read_csv(\"../Prix_metre/X_train_avec_prix.csv\")\n",
    "\n",
    "X = X_AC.iloc[:len(X_train_site),:]\n",
    "\n",
    "y = pd.read_csv(\"./y_train_OXxrJt1.csv\")\n",
    "y = y[[\"price\"]] # On ne garde que la colonne price, un supprime la colonne qui porte sur le numéro de l'annonce\n",
    "\n",
    "X.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> On aura régulièrement besoin d'une colone \"département\". Nous la créons donc dès maintenant sur l'entiereté du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_AC[\"departement\"]=X_AC[\"postal_code\"]\n",
    "X_AC[\"departement\"][X_AC[\"departement\"] < 10000]=X_AC[\"departement\"].astype(str).str[:1].astype(int)\n",
    "X_AC[\"departement\"][X_AC[\"departement\"] >= 10000]=X_AC[\"departement\"].astype(str).str[:2].astype(int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Récupération de données à partir d'autres datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset de l'INSEE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> On récupère le dataset et on effectue quelques traitements dessus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insee = pd.read_excel(\"Data_auxiliaires/MDB-INSEE-V2.xls\")\n",
    "insee.columns\n",
    "\n",
    "# Suppression des lignes où CODGEO contient des lettres\n",
    "insee = insee[insee[\"CODGEO\"].apply(lambda x: x.isnumeric())]\n",
    "\n",
    "# On convertit CODGEO en int\n",
    "insee[\"CODGEO\"] = insee[\"CODGEO\"].astype(int)\n",
    "\n",
    "# On drop les colomnes de types \"object\"\n",
    "for col in insee.columns:\n",
    "    if insee[col].dtype == \"object\":\n",
    "        insee = insee.drop(col, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Il était au départ prévu d'effectuer une moyenne par département des colonnes afin de pouvoir rapprocher les \"CODGEO\" aux valeurs des départements, cependant les résultats n'étaient pas concluants"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> On effectue une jointure entre notre dataset X_AC et celui de l'INSEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_AC = X_AC.merge(insee, left_on=\"postal_code\", right_on=\"CODGEO\", how=\"left\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jointure avec un dataset du gouvernement portant sur les chiffres de 2019 pour l'immobilier et sur certains de 1819"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_prix = pd.read_csv(\"C:/Users/maila/Documents/Centrale Nantes/EI2/INFOIA/STASC/Data_Challenge/STASC/Prix_metre/prixm2-loyer-communes-2019.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Le dataset étant particulier, de nombreux traitements ont du être effectués à la main"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> On recherche les lignes dont le nom de la commune est 'VILLENEUVE-LOUBET' ou 'VILLENEUVE LOUBET' afin de détecter des doublons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_prix[X_prix['NOM_COM_M'] == 'VILLENEUVE-LOUBET']\n",
    "\n",
    "X_prix[X_prix['NOM_COM_M'] == 'VILLENEUVE LOUBET']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> On supprime le mot \"ARRONDISSEMENT\" de la colonne NOM_COM_M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_prix['NOM_COM_M'] = X_prix['NOM_COM_M'].str.replace('ARRONDISSEMENT', '')\n",
    "X_prix['NOM_COM_M'] = X_prix['NOM_COM_M'].str.replace('1E', '1eme')\n",
    "X_prix['NOM_COM_M'] = X_prix['NOM_COM_M'].str.replace('2E', '2eme')\n",
    "X_prix['NOM_COM_M'] = X_prix['NOM_COM_M'].str.replace('3E', '3eme')\n",
    "X_prix['NOM_COM_M'] = X_prix['NOM_COM_M'].str.replace('4E', '4eme')\n",
    "X_prix['NOM_COM_M'] = X_prix['NOM_COM_M'].str.replace('5E', '5eme')\n",
    "X_prix['NOM_COM_M'] = X_prix['NOM_COM_M'].str.replace('6E', '6eme')\n",
    "X_prix['NOM_COM_M'] = X_prix['NOM_COM_M'].str.replace('7E', '7eme')\n",
    "X_prix['NOM_COM_M'] = X_prix['NOM_COM_M'].str.replace('8E', '8eme')\n",
    "X_prix['NOM_COM_M'] = X_prix['NOM_COM_M'].str.replace('9E', '9eme')\n",
    "X_prix['NOM_COM_M'] = X_prix['NOM_COM_M'].str.replace('10E', '10eme')\n",
    "X_prix['NOM_COM_M'] = X_prix['NOM_COM_M'].str.replace('11E', '11eme')\n",
    "X_prix['NOM_COM_M'] = X_prix['NOM_COM_M'].str.replace('12E', '12eme')\n",
    "X_prix['NOM_COM_M'] = X_prix['NOM_COM_M'].str.replace('13E', '13eme')\n",
    "X_prix['NOM_COM_M'] = X_prix['NOM_COM_M'].str.replace('14E', '14eme')\n",
    "X_prix['NOM_COM_M'] = X_prix['NOM_COM_M'].str.replace('15E', '15eme')\n",
    "X_prix['NOM_COM_M'] = X_prix['NOM_COM_M'].str.replace('16E', '16eme')\n",
    "X_prix['NOM_COM_M'] = X_prix['NOM_COM_M'].str.replace('17E', '17eme')\n",
    "X_prix['NOM_COM_M'] = X_prix['NOM_COM_M'].str.replace('18E', '18eme')\n",
    "X_prix['NOM_COM_M'] = X_prix['NOM_COM_M'].str.replace('19E', '19eme')\n",
    "X_prix['NOM_COM_M'] = X_prix['NOM_COM_M'].str.replace('20E', '20eme')\n",
    "\n",
    "X_prix['NOM_COM_M'] = X_prix['NOM_COM_M'].str.replace('1eme ', '1eme')\n",
    "X_prix['NOM_COM_M'] = X_prix['NOM_COM_M'].str.replace('2eme ', '2eme')\n",
    "X_prix['NOM_COM_M'] = X_prix['NOM_COM_M'].str.replace('3eme ', '3eme')\n",
    "X_prix['NOM_COM_M'] = X_prix['NOM_COM_M'].str.replace('4eme ', '4eme')\n",
    "X_prix['NOM_COM_M'] = X_prix['NOM_COM_M'].str.replace('5eme', '5eme')\n",
    "X_prix['NOM_COM_M'] = X_prix['NOM_COM_M'].str.replace('6eme ', '6eme')\n",
    "X_prix['NOM_COM_M'] = X_prix['NOM_COM_M'].str.replace('7eme ', '7eme')\n",
    "X_prix['NOM_COM_M'] = X_prix['NOM_COM_M'].str.replace('8eme ', '8eme')\n",
    "X_prix['NOM_COM_M'] = X_prix['NOM_COM_M'].str.replace('9eme ', '9eme')\n",
    "X_prix['NOM_COM_M'] = X_prix['NOM_COM_M'].str.replace('10eme ', '10eme')\n",
    "X_prix['NOM_COM_M'] = X_prix['NOM_COM_M'].str.replace('11eme ', '11eme')\n",
    "X_prix['NOM_COM_M'] = X_prix['NOM_COM_M'].str.replace('12eme ', '12eme')\n",
    "X_prix['NOM_COM_M'] = X_prix['NOM_COM_M'].str.replace('13eme ', '13eme')\n",
    "X_prix['NOM_COM_M'] = X_prix['NOM_COM_M'].str.replace('14eme ', '14eme')\n",
    "X_prix['NOM_COM_M'] = X_prix['NOM_COM_M'].str.replace('15eme ', '15eme')\n",
    "X_prix['NOM_COM_M'] = X_prix['NOM_COM_M'].str.replace('16eme ', '16eme')\n",
    "X_prix['NOM_COM_M'] = X_prix['NOM_COM_M'].str.replace('17eme ', '17eme')\n",
    "X_prix['NOM_COM_M'] = X_prix['NOM_COM_M'].str.replace('18eme ', '18eme')\n",
    "X_prix['NOM_COM_M'] = X_prix['NOM_COM_M'].str.replace('19eme ', '19eme')\n",
    "X_prix['NOM_COM_M'] = X_prix['NOM_COM_M'].str.replace('20eme ', '20eme')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> On passe en minuscule la colonne 'NOM_COM_M' et on enlève les \"-\" entre les mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_prix['NOM_COM_M'] = X_prix['NOM_COM_M'].str.lower()\n",
    "X_prix['NOM_COM_M'] = X_prix['NOM_COM_M'].str.replace('-', ' ')\n",
    "X['city'] = X['city'].str.replace('-', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_prix.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> On effectue une jointure des deux tables afin de ne garder que les lignes de X_prix qui ont une correspondance dans X_AC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_AC = X_AC.merge(X_prix, how='left', left_on='city', right_on='NOM_COM_M')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> On supprime les doublons et on vérifie ensuite le nombre de valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_AC.shape)\n",
    "print(X_AC[[\"id_annonce\"]].duplicated().sum())\n",
    "X_AC = X_AC.drop_duplicates(subset=[\"id_annonce\"], keep=\"first\")\n",
    "\n",
    "X_AC.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Pour les 10% sans valeurs, on met la valeur moyenne du département"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_AC['Prixm2'].fillna(X_AC.groupby('departement')['Prixm2'].transform('mean'), inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> S'il reste encore des valeurs manquantes, on remplit avec la moyenne globale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_AC['Prixm2'].fillna(X_AC['Prixm2'].mean(), inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rajout de features en tenant compte de l'inflation à l'aide d'un autre dataset de l'INSEE\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> On crée un peu en avance une feature de nearly_price (on la supprimera après afin de ne pas intérférer avec la partie de Feature Engineering située plus loin dans le notebook) à l'aide du dataset précédent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_AC[\"nearly_price\"]=X_AC[\"PrixMoyen_M2\"]*X_AC[\"size\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> On initialise une colonne \"nearly_price_inflation\" avec None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_AC[\"nearly_price_inflation\"]=None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> On traite ensuite cette colonne en fonction du type de bien et du département. Toutes les données des taux d'inflation ont été ajouté à la main ici, car le fichier de l'INSEE correspondant était peu exploitable en Python directement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "departements_dataset = X_AC[\"departement\"].unique().tolist()\n",
    "# departements présents explicitement dans la base de données de l'INSEE\n",
    "departements = [75,77,78,91,92,93,94,95,4,5,6,13,83,84,1,3,7,15,26,38,42,43,63,69,73,74,2,59,60,62,80]\n",
    "taux_inflation_apparts = [1.034802784,1.067698259,1.068328717,1.070949185,1.08037225,1.094952951,1.089005236,1.074144487,1.066918002,1.066918002,1.066918002,1.066918002,1.066918002,1.066918002,1.114111498,1.114111498,1.114111498,1.114111498,1.114111498,1.114111498,1.114111498,1.114111498,1.114111498,1.114111498,1.114111498,1.114111498,1.110377358,1.110377358,1.110377358,1.110377358,1.110377358]\n",
    "taux_inflation_maison = [1.034802784,1.092380952,1.075418994,1.080449018,1.097797357,1.094977169,1.099188458,1.086997194,1.083409716,1.083409716,1.083409716,1.083409716,1.083409716,1.083409716,1.095829636,1.095829636,1.095829636,1.095829636,1.095829636,1.095829636,1.095829636,1.095829636,1.095829636,1.095829636,1.095829636,1.095829636,1.088180113,1.088180113,1.088180113,1.088180113,1.088180113]\n",
    "\n",
    "\n",
    "\n",
    "# On remplit les valeurs pour les maisons et les apparts dont on a les numéros\n",
    "for i in range(len(departements)):\n",
    "    X_AC[\"nearly_price_inflation\"][(X_AC[\"property_type\"] == \"appartement\") & (X_AC[\"departement\"] == departements[i])] = X_AC[\"nearly_price\"][(X_AC[\"property_type\"] == \"appartement\") & (X_AC[\"departement\"] == departements[i])] * taux_inflation_apparts[i]\n",
    "    X_AC[\"nearly_price_inflation\"][(X_AC[\"property_type\"] == \"maison\") & (X_AC[\"departement\"] == departements[i])] = X_AC[\"nearly_price\"][(X_AC[\"property_type\"] == \"maison\") & (X_AC[\"departement\"] == departements[i])] * taux_inflation_maison[i]\n",
    "\n",
    "province_apparts = 1.106019766\n",
    "province_maisons = 1.101083032\n",
    "\n",
    "departements_not_in_INSEE = [dep for dep in departements_dataset if dep not in departements]\n",
    "\n",
    "for dep in departements_not_in_INSEE:\n",
    "# on remplit les valeurs pour les maisons et les apparts dont on a pas les numéros (donc \"Province\")\n",
    "    X_AC[\"nearly_price_inflation\"][(X_AC[\"property_type\"] == \"appartement\") & (X_AC[\"departement\"] == dep)] = X_AC[\"nearly_price\"][(X_AC[\"property_type\"] == \"appartement\") & (X_AC[\"departement\"] == dep)] * province_apparts\n",
    "    X_AC[\"nearly_price_inflation\"][(X_AC[\"property_type\"] == \"maison\") & (X_AC[\"departement\"] == dep)] = X_AC[\"nearly_price\"][(X_AC[\"property_type\"] == \"maison\") & (X_AC[\"departement\"] == dep)] * province_maisons\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Pour le reste des types (non présents dans le dataset), on remplit par la valeur moyenne du département :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"nearly_price_inflation\"] = X.groupby(\"departement\")[\"nearly_price_inflation\"].transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_AC = X_AC.drop(columns=[\"nearly_price\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création des scores des images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Nous avons adapté un modèle de Deep Learning pré entrainée nommée RoomNet à la détection de pièces, pour pouvoir l'utiliser sur nos images. L'adaptation n'est pas visible car celle-ci a été effectué directement dans les fichiers d'origines de RoomNet récupérés sur Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS = ['Backyard', 'Bathroom', 'Bedroom', 'Frontyard', 'Kitchen', 'LivingRoom']\n",
    "POND = [2, 0.5, 1, 1.5, 1.5, 2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Traitement des fichiers excels obtenus avec RoomNet pour obtenir un unique Dataframe avec un score en fonction des images pour chaque annonce. Chaque pièce se voit attribuer un coefficient d'importance qui est ensuite multiplié par le nombre d'images de la pièce en question avant d'être sommé pour chaque annonce aux autres scores des autres images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(id_dataset):\n",
    "    if id_dataset == \"train\":\n",
    "        path_to_csv = \"C:/Users/maila/Documents/Centrale Nantes/EI2/INFOIA/STASC/Data_Challenge/_classified/train\"\n",
    "\n",
    "    elif id_dataset == \"test\":\n",
    "        path_to_csv = \"C:/Users/maila/Documents/Centrale Nantes/EI2/INFOIA/STASC/Data_Challenge/_classified/test\"\n",
    "\n",
    "    list_of_files = os.listdir(path_to_csv)  # On récupère la liste de l'ensemble des fichiers .csv créées à l'aide du modèle de DL RoomNet \n",
    "    \n",
    "    # création du dictionnaire final, qui contiendra toutes les données des excels et les scores (sans les valeurs de fiabilité)\n",
    "    donnees_finales = pd.DataFrame({'id_annonce':[],'Backyard' : [], 'Bathroom' : [], 'Bedroom' : [], 'Frontyard' : [],'Kitchen' : [], 'LivingRoom' : [],'score':[]})\n",
    "    dict_operations = {'Value_Count': ('PREDICTED_LABEL', 'count'), 'average_fiability': ('Unnamed: 2', 'mean')} # dictionnaire qui contient les opérations à effectuer sur les colonnes des dataframes qu'on récup\n",
    "\n",
    "    i=0\n",
    "    for file in list_of_files:\n",
    "        if i%1000 == 0: # compteur pour suivre l'avancement du traitement\n",
    "            print(i)\n",
    "        i = i+1\n",
    "        A = pd.read_excel(path_to_csv + \"/\" + file)\n",
    "        elem_presents = A.groupby('PREDICTED_LABEL', as_index=False).agg(**dict_operations) # on crée un dataframe avec le count des différentes classes et la fiabilité moyenne par classe\n",
    "\n",
    "        dict_temp = {} # création du dictionnaire temporaire qui contiendra la valeurs avant de les mettre dans le dataframe final\n",
    "        score = 0 # initialisation du score\n",
    "        for elem in elem_presents['PREDICTED_LABEL']:\n",
    "            number_of_elem = elem_presents['Value_Count'][elem_presents['PREDICTED_LABEL'] == elem].tolist()[0] # on récupère le nombre d'éléments de la classe\n",
    "            average_fiability = elem_presents['average_fiability'][elem_presents['PREDICTED_LABEL'] == elem].tolist()[0] # on récupère la fiabilité moyenne de la classe\n",
    "            dict_temp[elem] = [number_of_elem] # on met le nombre d'éléments de la classe dans le dictionnaire\n",
    "            score = score + number_of_elem*POND[CLASS.index(elem)]*average_fiability # on calcule le score en multipliant notre pondération et la fiabilité moyenne de la classe\n",
    "\n",
    "        score = [score]\n",
    "        dict_temp['score'] = score # on ajoute le score au dictionnaire\n",
    "        dict_temp['id_annonce'] = file[4:-12] # on ajoute l'id de l'annonce au dictionnaire\n",
    "\n",
    "        temp = pd.DataFrame(dict_temp)\n",
    "        donnees_finales = pd.concat([donnees_finales,temp],ignore_index=True)\n",
    "    return(donnees_finales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donnees_finales_X=train = score(\"train\")\n",
    "donnees_finales_X.to_csv('donnees_finales_images_train.csv', index=False)\n",
    "\n",
    "donnees_finales_X_test=test = score(\"test\")\n",
    "donnees_finales_X_test.to_csv('donnees_finales_images_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_test = pd.read_csv('donnees_finales_images_test.csv')\n",
    "image_train = pd.read_csv('donnees_finales_images_train.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Concaténation de test et train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = pd.concat([image_test,image_train],ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Jointure de X_AC et image grâce à la colonne \"id_annonce\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_AC = pd.merge(X, image, on='id_annonce', how='left')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second traitement des images (non retenu au final)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> On commence par spécifier les PATHs vers les images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listes des dossiers dans le dossier train\n",
    "FOLDER_TRAIN = 'C:/Users/maila/Documents/Centrale Nantes/EI2/INFOIA/STASC/Data_Challenge/reduced_images_ILB/reduced_images/train'\n",
    "list_folders_train = os.listdir(FOLDER_TRAIN)\n",
    "# liste des dossiers dans le dossier test\n",
    "FOLDER_TEST = 'C:/Users/maila/Documents/Centrale Nantes/EI2/INFOIA/STASC/Data_Challenge/reduced_images_ILB/reduced_images/test'\n",
    "list_folders_test = os.listdir(FOLDER_TEST)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> On calcule la moyenne des pixels en passant en teintes de gris ainsi que le premier et le troisième quartiles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traitement_images(Folder,list_folders):\n",
    "    futur_dataset = {\"id_annonce\" : [],\"gray_mean\" : [], \"quartile_inf\" : [], \"quartile_sup\" : []}\n",
    "\n",
    "    compteur = 0\n",
    "    for dossier in list_folders :\n",
    "        compteur += 1\n",
    "        # création d'un compteur pour voir l'avancement\n",
    "        if compteur % 1000 == 0 :\n",
    "            print(compteur)\n",
    "        # path absolue vers le folder\n",
    "        folder_path = Folder + \"/\" + dossier\n",
    "        list_images = os.listdir(folder_path)\n",
    "        for image in list_images :\n",
    "            quartile_inf_image = []\n",
    "            quartile_sup_image = []\n",
    "            moy_image = []\n",
    "\n",
    "            image_path = folder_path + \"/\" + image\n",
    "            img = cv2.imread(image_path)\n",
    "            # On passe l'image en gris\n",
    "            gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            # moyenne des pixels de l'image\n",
    "            moy_image.append(gray_img.mean())\n",
    "            # On calcule les quartiles de l'image\n",
    "            quartiles = np.quantile(gray_img, [0.25, 0.75])\n",
    "            quartile_inf_image.append(quartiles[0])\n",
    "            quartile_sup_image.append(quartiles[1])\n",
    "            # On ajoute l'image au futur dataset avec son id d'annonce\n",
    "        futur_dataset[\"id_annonce\"].append(dossier[4:])\n",
    "        futur_dataset[\"gray_mean\"].append(mean(moy_image))\n",
    "        futur_dataset[\"quartile_inf\"].append(mean(quartile_inf_image))\n",
    "        futur_dataset[\"quartile_sup\"].append(mean(quartile_sup_image))\n",
    "    return(futur_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traitement_images_train = traitement_images(FOLDER_TRAIN,list_folders_train)\n",
    "traitement_images_test = traitement_images(FOLDER_TEST,list_folders_test)\n",
    "\n",
    "traitement_global = pd.concat([traitement_images_train,traitement_images_test],ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etudes des colomnes à valeurs non numériques"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> On étudie pour l'instant seulement le dataset de train, car on possède les valeurs des prix correspondants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = (X.dtypes == 'object')    # pour avoir la liste des colonnes avec comme type \"objet\"\n",
    "object_cols = list(s[s].index)\n",
    "\n",
    "# On compte le nombre de valeurs différentes par colonne (pour après faire possiblement du One-Hot Encoding)\n",
    "nb_val_col = {}\n",
    "nb_val_manquantes = {}\n",
    "ratio_val_manquantes = {}\n",
    "for col in object_cols:\n",
    "    nb_val_col[col]=[len(X[f\"{col}\"].value_counts())]\n",
    "    nb_val_manquantes[col]=X[f\"{col}\"].isnull().sum() #on compte le nombre de valeurs manquantes pour savoir s'il faut garder la colonne\n",
    "    ratio_val_manquantes[col]=nb_val_manquantes[col]/len(X[f\"{col}\"]) #on calcule le ratio de valeurs manquantes\n",
    "\n",
    "nb_val_col_df = pd.DataFrame(nb_val_col.items(),columns = [\"Nom Colonne\",\"Nb de valeurs différentes\"])\n",
    "nb_val_manquantes = pd.DataFrame(nb_val_manquantes.items(),columns = [\"Nom Colonne\",\"Nb de valeurs manquantes\"])\n",
    "ratio_val_manquantes = pd.DataFrame(ratio_val_manquantes.items(),columns = [\"Nom Colonne\",\"Ratio de valeurs manquantes\"])\n",
    "\n",
    "nb_val_col_df[\"Nb de valeurs manquantes\"]=nb_val_manquantes[[\"Nb de valeurs manquantes\"]]\n",
    "nb_val_col_df['Ratio de valeurs manquantes']=ratio_val_manquantes[[\"Ratio de valeurs manquantes\"]]\n",
    "\n",
    "nb_val_col_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> On décide donc de drop exposition car il manque plus de 75% des valeurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(columns = [\"exposition\"],axis=1)\n",
    "X.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ghg_category et energy_performance_category correspondent à la même information que ghg_value et energy_performance_value mais en moins précis donc on les enlève  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(columns=['ghg_category', 'energy_performance_category'], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Pour floor et land_size on peut penser que les valeurs manquantes correspondent à des maisons pour floor et à des appartements pour land_size donc on les remplace par 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['floor'].fillna(0, inplace=True)\n",
    "X['land_size'].fillna(0, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Pour le moment, on décide d'appliquer du Label Encoder (idée Félix) à \"city\" et un One Hot Encoding à \"property_type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding for property_type\n",
    "X = pd.get_dummies(X, columns=['property_type'], drop_first=False)\n",
    "\n",
    "# Label encoding for city\n",
    "le = LabelEncoder()\n",
    "X['city'] = le.fit_transform(X['city'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On crée une colonne département dont on aura besoin pour remplacer les valeurs nulles ensuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des valeurs des départements plutôt que des codes postaux (en séparant les départements à 4 chiffres des départements à 5 chiffres)\n",
    "X[\"departement\"]=X[\"postal_code\"]\n",
    "X[\"departement\"][X[\"departement\"] < 10000]=X[\"departement\"].astype(str).str[:1].astype(int)\n",
    "X[\"departement\"][X[\"departement\"] >= 10000]=X[\"departement\"].astype(str).str[:2].astype(int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On s'occupe maintenant des valeurs nulles de toutes les colonnnes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colomnes = X.columns\n",
    "nb_lignes = X.shape[0]\n",
    "ratio_val_manquantes = {}\n",
    "\n",
    "\n",
    "for col in colomnes:\n",
    "    ratio_val_manquantes[col] = X[col].isnull().sum()/nb_lignes\n",
    "\n",
    "ratio_val_manquantes = pd.DataFrame(ratio_val_manquantes.items(),columns = [\"Nom Colonne\",\"Ratio de valeurs manquantes\"])\n",
    "ratio_val_manquantes.sort_values(by = \"Ratio de valeurs manquantes\",ascending = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> On remplit les données manquantes avec pour certaines colonnes la moyenne du code postal, et pour d'autres la moyenne par département"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_incomplete(X):   \n",
    "    Features_incomplete = []\n",
    "    for x in X.columns:\n",
    "        if X[x].isnull().sum()/len(X[x])*100 > 0:\n",
    "            Features_incomplete.append(x)\n",
    "    return Features_incomplete\n",
    "\n",
    "# Tentatives de remplir les valeurs manquantes de ces colonnes par la valeur moyenne pour le code postal associé\n",
    "X[\"POPULATION_x\"].fillna(X.groupby(\"postal_code\")[\"POPULATION_x\"].transform(\"mean\"), inplace=True)\n",
    "X[\"Nb_Transac\"].fillna(X.groupby(\"postal_code\")[\"Nb_Transac\"].transform(\"mean\"), inplace=True)\n",
    "X[\"Nb_Ventes\"].fillna(X.groupby(\"postal_code\")[\"Nb_Ventes\"].transform(\"mean\"), inplace=True)\n",
    "    \n",
    "for x in features_incomplete(X):\n",
    "# On remplit le reste par la valeur moyenne du département\n",
    "    X[x].fillna(X.groupby(\"departement\")[x].transform(\"mean\"), inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etude de la corrélation entre les features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> On regarde la corrélation entre les features (en ajoutant le prix à prédire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_with_price = X.copy()\n",
    "X_with_price[\"price\"] = np.log1p(y[\"price\"])\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "cor = X_with_price.corr()\n",
    "sns.heatmap(cor, annot=False, cmap=plt.cm.Reds)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> On remarque sur la matrice de corrélation, la forte corrélation entre le prix et nb_terraces, has_a_cellar, nb_photos, nb_rooms, nb_bedrooms, nb_bathrooms, postal_code. On refait une matrice de corrélation juste avec ses variables :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_correlation = X_with_price[[\"nb_terraces\", \"has_a_cellar\",\"nb_photos\",\"nb_rooms\",\"nb_bedrooms\",\"nb_bathrooms\",\"postal_code\",\"price\",\"departement\"]]\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "cor = X_correlation.corr()\n",
    "sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> On essaie alors de créer d'autres variables à partir de celles-ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = X_correlation[\"price\"]\n",
    "X_correlation = X_correlation.drop(columns = [\"price\"],axis=1)\n",
    "X_correlation[\"Sum_bedrooms_rooms\"] = X_correlation[\"nb_rooms\"]+X_correlation[\"nb_bedrooms\"]\n",
    "X_correlation[\"Sum_bathrooms_rooms\"] = X_correlation[\"nb_rooms\"]+X_correlation[\"nb_bathrooms\"]\n",
    "X_correlation[\"Diff_bedrooms_bathrooms\"] = X_correlation[\"nb_bathrooms\"]-X_correlation[\"nb_bedrooms\"]\n",
    "X_correlation[\"Ratio_size_rooms\"] = X[\"size\"]/X_correlation[\"nb_rooms\"]\n",
    "\n",
    "# Rajout du prix pour l'étude de corrélation\n",
    "X_correlation[\"price\"] = np.log1p(y[\"price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "cor = X_correlation.corr()\n",
    "sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Dans les tentatives de rajout, les colonnes qui semblent intéressantes sont : Sum_bedrooms_rooms, Sum_bathrooms_rooms, Diff_bedrooms_bathrooms et departement. On les rajoute alors à X et on recalcule une heatmap :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"Sum_bedrooms_rooms\"] = X_correlation[\"Sum_bedrooms_rooms\"]\n",
    "X[\"Sum_bathrooms_rooms\"] = X_correlation[\"Sum_bathrooms_rooms\"]\n",
    "X[\"Diff_bedrooms_bathrooms\"] = X_correlation[\"Diff_bedrooms_bathrooms\"]\n",
    "X[\"departement\"] = X_correlation[\"departement\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "cor = X.corr()\n",
    "sns.heatmap(cor, annot=False, cmap=plt.cm.Reds)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> On vérifie le taux de données manquantes dans les colonnes nouvellement créées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colomnes = X.columns[X.isnull().any()]\n",
    "nb_lignes = X.shape[0]\n",
    "ratio_val_manquantes = {}\n",
    "\n",
    "\n",
    "for col in colomnes:\n",
    "    ratio_val_manquantes[col] = X[col].isnull().sum()/nb_lignes\n",
    "\n",
    "ratio_val_manquantes = pd.DataFrame(ratio_val_manquantes.items(),columns = [\"Nom Colonne\",\"Ratio de valeurs manquantes\"])\n",
    "ratio_val_manquantes.sort_values(by = \"Ratio de valeurs manquantes\",ascending = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajout de colonnes de prix approximés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"nearly_price\"]=X[\"PrixMoyen_M2\"]*X[\"size\"]\n",
    "X[\"nearly_price_1819\"]=X[\"PrixMoyen_M2_1819\"]*X[\"size\"]\n",
    "\n",
    "for x in features_incomplete(X):\n",
    "    X[x].fillna(X.groupby(\"departement\")[x].transform(\"mean\"), inplace=True)\n",
    "\n",
    "# fill missing values with mean column values (il reste des valeurs manquantes dans 'nearly_price_1819')\n",
    "X.fillna(X.mean(), inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajout de racines de prix et de surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"racine_size\"] = np.sqrt(X[\"size\"])\n",
    "X[\"racine_land_size\"] = np.sqrt(X[\"land_size\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajout du score des images (obtenus par DL avec RoomNet)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> On merge les deux tables pour rajouter le score sur X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_train = pd.read_csv(\"C:/Users/maila/Documents/Centrale Nantes/EI2/INFOIA/STASC/Data_Challenge/STASC/traitement_photos/donnees_finales_images_train.csv\")\n",
    "images_train_final = images_train[[\"id_annonce\",\"score\"]] # On ne garde que les colonnes qui nous intéressent\n",
    "\n",
    "X = X.merge(images_train_final, how='left', left_on='id_annonce', right_on='id_annonce')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Et pour finir, on drop la colonne correspondant aux ids des annonces"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création de clusters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cluster des prix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> On calcule des clusters pour les prix afin de me visualiser leur répartition. Cependant, il ne faut pas utiliser ce Cluster comme feature, car il a directement été créé à partir de la target qu'est le prix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# création d'un mini dataset avec les coordonnées et le prix pour pouvoir faire un clustering\n",
    "X_cluster_prix = X[[\"approximate_latitude\", \"approximate_longitude\"]]\n",
    "X_cluster_prix[\"price\"]=y[[\"price\"]]\n",
    "# print(X_cluster_prix.head()) \n",
    "\n",
    "kmeans = KMeans(n_clusters=6)\n",
    "X_cluster_prix[\"Cluster\"] = kmeans.fit_predict(X_cluster_prix)\n",
    "X_cluster_prix[\"Cluster\"] = X_cluster_prix[\"Cluster\"].astype(\"category\")\n",
    "\n",
    "X_cluster_prix.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Affichage graphique du clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x=\"approximate_longitude\", y=\"approximate_latitude\", hue = \"Cluster\", data=X_cluster_prix, height=6, palette = sns.color_palette(\"flare\",n_colors = 6))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Création d'un clustering géographique avec la taille des biens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cluster_size = X[[\"approximate_longitude\",\"approximate_latitude\",\"nearly_price\"]]\n",
    "X_cluster_size[\"nearly_price\"]=np.log1p(X_cluster_size[\"nearly_price\"])\n",
    "# print(X_cluster_prix.head())\n",
    "\n",
    "N=15\n",
    "kmeans = KMeans(n_clusters=N)\n",
    "X_cluster_size[\"Cluster\"] = kmeans.fit_predict(X_cluster_size)\n",
    "X_cluster_size[\"Cluster\"] = X_cluster_size[\"Cluster\"].astype(\"category\")\n",
    "# print(X_cluster_size.head(10))\n",
    "\n",
    "sns.relplot(x=\"approximate_longitude\", y=\"approximate_latitude\", hue = \"Cluster\", s=25,data=X_cluster_size, height=N, palette = sns.color_palette(\"Spectral\",n_colors = N))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Engineering final"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Le clustering semblant intéressant par étude de la feature importance, on va l'ajouter au dataset en même temps que différentes fonctions de preprocessing. Pour cela, on charge l'ensemble des datasets (train et test), soit X_avant_coupure et on applique directement l'ensemble des traitements dessus en créant deux fonctions qui résument le feature engineering et le preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_incomplete(X):   \n",
    "    Features_incomplete = []\n",
    "    for x in X.columns:\n",
    "        if X[x].isnull().sum()/len(X[x])*100 > 0:\n",
    "            Features_incomplete.append(x)\n",
    "    return Features_incomplete\n",
    "\n",
    "def FE(X):\n",
    "    X.drop(columns = [\"exposition\"],inplace=True)\n",
    "    X.drop(columns=['ghg_category', 'energy_performance_category'], inplace=True)\n",
    "    X['floor'].fillna(0, inplace=True)\n",
    "    X['land_size'].fillna(0, inplace=True)\n",
    "\n",
    "    #one hot encoding for property_type\n",
    "    X = pd.get_dummies(X, columns=['property_type'], drop_first=False)\n",
    "\n",
    "    # Label encoding for city\n",
    "    #le = LabelEncoder()\n",
    "    #X['city'] = le.fit_transform(X['city'])\n",
    "    \n",
    "    # Frequency encoding for city\n",
    "    X['city'] = X['city'].map(X['city'].value_counts())\n",
    "\n",
    "\n",
    "    # Calcul des valeurs des départements plutôt que des codes postaux (en séparant les départements à 4 chiffres des départements à 5 chiffres)\n",
    "    X[\"departement\"]=X[\"postal_code\"]\n",
    "    X[\"departement\"][X[\"departement\"] < 10000]=X[\"departement\"].astype(str).str[:1].astype(int)\n",
    "    X[\"departement\"][X[\"departement\"] >= 10000]=X[\"departement\"].astype(str).str[:2].astype(int)\n",
    "\n",
    "    # Tentatives de remplir les valeurs manquantes de ces colonnes par la valeur moyenne pour le code postal associé\n",
    "    X[\"POPULATION_x\"].fillna(X.groupby(\"postal_code\")[\"POPULATION_x\"].transform(\"mean\"), inplace=True)\n",
    "    X[\"Nb_Transac\"].fillna(X.groupby(\"postal_code\")[\"Nb_Transac\"].transform(\"mean\"), inplace=True)\n",
    "    X[\"Nb_Ventes\"].fillna(X.groupby(\"postal_code\")[\"Nb_Ventes\"].transform(\"mean\"), inplace=True)\n",
    "    \n",
    "    X[\"nearly_price\"]=X[\"PrixMoyen_M2\"]*X[\"size\"]\n",
    "    X[\"nearly_price_1819\"]=X[\"PrixMoyen_M2_1819\"]*X[\"size\"]\n",
    "    X[\"prix_metre\"] = np.sqrt(X[\"PrixMoyen_M2\"])\n",
    "    X[\"prix_metre_1819\"] = np.sqrt(X[\"PrixMoyen_M2_1819\"])\n",
    "    #------------------------------\n",
    "    X[\"prix_metre_2017\"] = np.sqrt(X[\"Prix_2017\"])\n",
    "    X[\"prix_metre_2018\"] = np.sqrt(X[\"Prix_2018\"])\n",
    "    X[\"racine_size\"] = np.sqrt(X[\"size\"])\n",
    "    X[\"racine_land_size\"] = np.sqrt(X[\"land_size\"])\n",
    "    \n",
    "    X[\"Somme_bedrooms_rooms\"] = X[\"nb_rooms\"]+X[\"nb_bedrooms\"]\n",
    "    X[\"Somme_bathrooms_rooms\"] = X[\"nb_rooms\"]+X[\"nb_bathrooms\"]\n",
    "    X[\"Diff_bedrooms_bathrooms\"] = X[\"nb_bathrooms\"]-X[\"nb_bedrooms\"]\n",
    "\n",
    "  \n",
    "    \n",
    "    \n",
    "\n",
    "    # Prendre l'entier de la longitude et de la latitude\n",
    "    X[\"approximate_longitude\"]=X[\"approximate_longitude\"].astype(int)\n",
    "    X[\"approximate_latitude\"]=X[\"approximate_latitude\"].astype(int)\n",
    "    \n",
    "    \n",
    "    for x in [\"approximate_longitude\",\"approximate_latitude\",\"nearly_price\"]:\n",
    "    # On remplit le reste par la valeur moyenne du département\n",
    "        X[x].fillna(X.groupby(\"departement\")[x].transform(\"mean\"), inplace=True)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    X = X.drop(columns = [\"id_annonce\"],axis=1)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(X_train, X_test):\n",
    "    scaler = MinMaxScaler()\n",
    "    for x in X_train.columns:\n",
    "        if X_train[x].dtype != 'object':\n",
    "            X_train[x] = scaler.fit_transform(X_train[x].values.reshape(-1,1))\n",
    "            X_test[x] = scaler.transform(X_test[x].values.reshape(-1,1))\n",
    "\n",
    "    # création d'un mini dataset avec les coordonnées et le prix pour pouvoir faire un clustering\n",
    "    X_cluster_size = X_train[[\"approximate_longitude\",\"approximate_latitude\",\"nearly_price\"]]\n",
    "\n",
    "    N=15\n",
    "    kmeans = KMeans(n_clusters=N)\n",
    "    X_cluster_size[\"Cluster\"] = kmeans.fit_predict(X_cluster_size)\n",
    "    X_cluster_size[\"Cluster\"] = X_cluster_size[\"Cluster\"].astype(\"category\")\n",
    "    X_train[\"Cluster\"]=X_cluster_size[\"Cluster\"]\n",
    "    X_test[\"Cluster\"]=kmeans.predict(X_test[[\"approximate_longitude\",\"approximate_latitude\",\"nearly_price\"]])\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    for x in X_train.columns:\n",
    "        if X_train[x].dtype != 'object':\n",
    "            X_train[x] = scaler.fit_transform(X_train[x].values.reshape(-1,1))\n",
    "            X_test[x] = scaler.transform(X_test[x].values.reshape(-1,1))\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_AC = FE(X_AC)\n",
    "X_train = X_AC.iloc[:len(X_train),:]\n",
    "X_test = X_AC.iloc[len(X_train):,:]\n",
    "X_train, X_test = preprocessing(X_train, X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Projet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63e22c97e4ed8d431fc724019dffa39fd8054b2d28cf3e66fed132b173246b98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
